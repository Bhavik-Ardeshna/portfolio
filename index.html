<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Bhavik Ardeshna</title>

    <meta name="author" content="Bhavik Ardeshna">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!--<link rel="icon" type="image/png" href="images/waterloo_crest.png"> -->
</head>

<body style="background-color: rgb(255, 255, 255);">
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"> 
                        <tbody>
                            <hr>
                            <div style="padding:12px;width:100%;vertical-align:middle;font-size:23px;">
                                <a style="font-size:19px" href="#Publications">Publicationsüìù</a> &nbsp/&nbsp 
                                <a style="font-size:19px" href="#Articles&Blogs">Articles & Blogs‚úç</a> &nbsp/&nbsp 
                                <a style="font-size:19px" href="#Projects">Projectsüíª</a> &nbsp/&nbsp 
                                <a style="font-size:19px" href="#Datasets">Datasetsüìö</a> 
                            </div>
                            <hr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Bhavik Ardeshna</name>
                                    </p>
                                    <p> <blockquote><b>I see a purposeful and versatile vocation in natural language processing, yet what I love most is its fusion of a scientific core with a semantic understanding of languages and also aligning linguistic and graphical understanding with an abstraction of multi-modular tactics.</b></blockquote>
                                    </p>
                                    <p>
                                        I am an author for Becoming Human: Artificial Intelligence Magazine featured on Google. My neoteric article <a href="#article1">[1]</a> and <a href="#article2">[2]</a> accentuates the outlook of the CQA and visiolinguistic learning. Additionally, I am opensource contributer at <a href="https://lets-unify.ai/" target="_blank">Ivy - The Unified Machine Learning Framework</a> and worked on nest functional API <a href="https://github.com/unifyai/ivy/pull/2002" target="_blank">#PR2002</a>. I have also published 14 multilingual transformer models in <a href="https://huggingface.co/bhavikardeshna" target="_blank">HuggingFaceü§ó</a> for seven divered languages for QA downstream task.

                                    </p>
                                    <p>During my time at DDU, I was engaged in research natural language, where I worked with <a
                                            href="https://scholar.google.com/citations?user=aEkOFcUAAAAJ&hl=en" target="_blank">Dr. Brijesh Bhatt</a> and <a href="https://scholar.google.com/citations?user=cXw0BegAAAAJ&hl=en" target="_blank">Prof. Harium Pandya</a> on QA downstream task for low-resource languages and investigate the efficacy for cross-lingual transfer using parameter-efficient adapter. Also generated the <em>GujaratiQASuite</em> benchmark for Gujarati QA system.</p>
                                    <p>
                                        Before that, I completed my SDE internship from <a href="https://heliconia.io/">Heliconia Solutions</a>, worked with automation team for developing utility tools for their product. I am also a <a href="https://www.kaggle.com/bhavikardeshna">Kaggle 3x Expert</a>, published many benchmarking datasets and notebooks which accentuate the concepts of Weights & Biases (wandb), Transformers, EDA, and various ensemble techniques..
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:ardeshnabhavik@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/Bhavik_Ardeshna.pdf">Resume</a> &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/bhavik-ardeshna-a4494a1b0/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                                        <a href="https://github.com/Bhavik-Ardeshna" target="_blank">GitHub</a>&nbsp/&nbsp
                                        <a href="https://www.kaggle.com/bhavikardeshna" target="_blank">Kaggle</a>&nbsp/&nbsp 
                                        <a href="https://medium.com/@ardeshnabhavik" target="_blank">Medium</a>&nbsp/&nbsp 
                                        <a href="https://huggingface.co/bhavikardeshna" target="_blank">HuggingFaceü§ó</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="image/1-Edit.png"><img
                                            style="width:100%;max-width:100%;border-radius: 46%;border: 4px solid rgb(255, 255, 255);"
                                            alt="profile photo" src="image/profile-image.jpeg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    
                    <br><hr>

                    <table id="Publications" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publicationsüìù</heading>
                                    <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/GujQA.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arxiv.org/abs/2112.09866" target="_blank">
                                        <papertitle>GujaratiQASuite: Novel Resources for Gujarati Question-Answering System</papertitle>
                                    </a>
                                    <br>
                                    <strong>Bhavik Ardeshna<sup>*</sup></strong>, <a
                                        href="https://www.researchgate.net/profile/Hariom-Pandya-2" target="_blank">Hariom
                                        Pandya</a><sup>*</sup>, <a
                                        href="https://scholar.google.co.in/citations?user=aEkOFcUAAAAJ&hl=en" target="_blank">Brijesh
                                        Bhatt</a>
                                    <br>
                                    Passed ARR and Under Review in <em>Springer Journal</em>
                                    <br>
                                    <!-- <u><a href="data/GATA.bib">bibtex</a></u> <u><a
                                            href="https://github.com/xingdi-eric-yuan/GATA-public">code</a></u> -->
                                    <p>
                                        Gujarati Question Answering Dataset (GujQA), which is the baseline QA benchmark for the Gujarati language. Additionally, we have offered thorough assessments of the GujQA using a variety of linguistic criteria and have seen encouraging outcomes. We believe that GujQA, GujAdapter, and
                                        Gujwiki will not only advance the study of the Gujarati language‚Äôs understudied QA but also
                                        provide a door to the cross-lingual study involving the languages of the typologically varied domain
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/caledip.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://aclanthology.org/2021.icon-main.66/" target="_blank">
                                        <papertitle>Cascading Adaptors to Leverage English Data to Improve Performance
                                            of Question Answering for Low-Resource Languages
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Bhavik Ardeshna<sup>*</sup></strong>, <a
                                        href="https://www.researchgate.net/profile/Hariom-Pandya-2" target="_blank">Hariom
                                        Pandya</a><sup>*</sup>, <a
                                        href="https://scholar.google.co.in/citations?user=aEkOFcUAAAAJ&hl=en" target="_blank">Brijesh
                                        Bhatt</a>
                                    <br>
                                    Accepted as a Thesis-Paper at <em>ACL Anthology (ICON-22)</em>
                                    <br>
                                    <u><a href="https://aclanthology.org/2021.icon-main.66/">paper</a></u> <u><a href="data/caledip.bib">bibtex</a></u> <u><a
                                            href="https://github.com/Bhavik-Ardeshna/Question-Answering-for-Low-Resource-Languages" target="_blank">code</a></u> <u><a href="data/caledip.pdf">pdf</a></u>
                                    <p>
                                        We have investigated the efficacy of cascading adapters with transformer models to leverage high-resource language to improve the performance of low-resource languages on the question answering task. We trained four variants of adapter combina-
                                        tions for - Hindi, Arabic, German, Spanish, English, Vietnamese, and Simplified Chinese languages. We demonstrated that by using the transformer model with the multi-task adapters, the performance can be improved for the downstream task.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <br><hr>

                    <table id="Articles&Blogs"
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Articles & Blogs‚úç</heading>
                                    <!-- <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p> -->
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr id="article1">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/coqa.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arvrjourney.com/basic-intuition-of-conversational-question-answering-systems-cqa-cf79bb5fa1d6" target="_blank">
                                        <papertitle>Basic intuition of Conversational Question Answering Systems (CQA)</papertitle>
                                    </a>
                                    <br>
                                    Published in <em>Becoming Human: Artificial Intelligence Magazine & Medium</em>
                                    <br>
                                    <u><a href="https://becominghuman.ai/" target="_blank">magazine</a></u> <u><a
                                            href="https://arvrjourney.com/basic-intuition-of-conversational-question-answering-systems-cqa-cf79bb5fa1d6" target="_blank">blog</a></u>

                                    <br><br>
                                    <quote><em>‚ÄúWe‚Äôre no longer teaching people how to communicate with systems, we‚Äôre teaching systems to communicate with people.‚Äù</em></quote>
                                    <p>
                                        The researcher has been working to develop an array of intelligent dialogue system that not only matches or surpasses a human‚Äôs level in carrying out an interactive conversation but also answers questions on a variety of topics.
                                    </p>
                                </td>
                            </tr>

                            <tr id="article2">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/visiolinguistic.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arvrjourney.com/semantic-alignment-of-linguistic-and-visual-understanding-using-multi-modal-transformer-86d64919ecc2" target="_blank">
                                        <papertitle>Semantic Alignment of Linguistic and Visual Understanding using Multi-modal Transformer
                                        </papertitle>
                                    </a>
                                    <br>
                                    Published in <em>Becoming Human: Artificial Intelligence Magazine & Medium</em>
                                    <br>
                                    <u><a href="https://becominghuman.ai/" target="_blank">magazine</a></u> <u><a
                                            href="https://arvrjourney.com/semantic-alignment-of-linguistic-and-visual-understanding-using-multi-modal-transformer-86d64919ecc2" target="_blank">blog</a></u> 
                                    <br><br>
                                    <quote><em>‚ÄúAlso, they don‚Äôt understand ‚Äî writing is language. The use of language. The language to create image, the language to create drama. It requires a skill of learning how to use language.‚Äù</em></quote>
                                    <p>
                                        Vision-language tasks, such as image captioning, visual question answering, and visual commonsense reasoning, serve as rich test-beds for evaluating the reasoning capabilities of visually informed systems.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <br><hr>

                    <table id="Projects"
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Projectsüíª</heading>
                                    <!-- <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p> -->
                                </td>
                            </tr>
                        </tbody>
                    </table>
                
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/vqa-project.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://github.com/Bhavik-Ardeshna/Visual-Question-Answering-MultiModular-Architecture" target="_blank">
                                        <papertitle>Visual Question-Answering MultiModular Architecture</papertitle>
                                    </a>
                                    <br>
                                    <em>Transformer, PyTorch, HuggingFace, Computer Vision, NLP</em>
                                    <br>
                                    <u><a href="https://github.com/Bhavik-Ardeshna/Visual-Question-Answering-MultiModular-Architecture" target="_blank">code</a></u> 
                                    <p>
                                        It emphasizes the featurization of image and question, feature fusion, and answer generation for the multimodal system for VisioLinguistic tasks, using different text and vision transformers to evaluate the efficacy of QA downstream task using a multimodular framework.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/qa-project.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://github.com/Bhavik-Ardeshna/Question-Answering-for-Low-Resource-Languages" target="_blank">
                                        <papertitle>Question-Answering for Low-Resource Languages
                                        </papertitle>
                                    </a>
                                    <br>
                                    <em>NLP, Language Modeling, Zero-Shot Learning, Adapters, HuggingFace</em>
                                    <br>
                                    <u><a href="https://github.com/Bhavik-Ardeshna/Question-Answering-for-Low-Resource-Languages" target="_blank">code</a></u> 
                                    <p>
                                        We trained four variants of adapter combinations for - Hindi, Arabic, German, Spanish, English, Vietnamese, and Simplified Chinese languages. We demonstrated that by using the transformer model with the multi-task adapters, the performance can be improved for the downstream task.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/pdfconvertor.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://github.com/Bhavik-Ardeshna/PDFConverter" target="_blank">
                                        <papertitle>PDFConverter
                                        </papertitle>
                                    </a>
                                    <br>
                                    <em>Flask, React, Jinja, Drag & Drop, Tailwind</em>
                                    <br>
                                    <u><a href="https://github.com/Bhavik-Ardeshna/PDFConverter" target="_blank">code</a></u> 
                                    <p>
                                        Features Provided By PDFConverter Tools
                                        <ul>
                                            <li>Rotate Pdf [90¬∞,180¬∞,270¬∞,360¬∞]</li>
                                            <li>Merge Pdf (Merger all uploaded or droped pdfs)</li>
                                            <li>Split Pdf (Split the pdf as per provided page numbers)</li>
                                            <li>Convert Pdf to [JPEG, PNG, HTML, Text]</li>
                                            <li>Crop Pdf & Download the modified pdf or zip</li>
                                        </ul>
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/creatively.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://github.com/Bhavik-Ardeshna/Creatively.io" target="_blank">
                                        <papertitle>Creatively.io
                                        </papertitle>
                                    </a>
                                    <br>
                                    <em>MERN, JWT-Auth, Tailwind, Canvas, API</em>
                                    <br>
                                    <u><a href="https://github.com/Bhavik-Ardeshna/Creatively.io" target="_blank">code</a></u> 
                                    <p>
                                        Creatively.io provides features to create their own canvas and with the help of its beautifully designed UI user can share to the world.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/instabook.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://github.com/Bhavik-Ardeshna/InstaBook" target="_blank">
                                        <papertitle>InstaBook</papertitle>
                                    </a>
                                    <br>
                                    <em>Django, SQLite, Web-Sockets, Tailwind</em>
                                    <br>
                                    <u><a href="https://github.com/Bhavik-Ardeshna/InstaBook" target="_blank">code</a></u> 
                                    <p>
                                        InstaBook was smartly created using Django and other tools such that it supports all features which are provided by Instagram. InstaBook is created to change social media communication and many new users would get attracted and use InstaBook and be socialize. 
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <br><hr>
                    <table id="Datasets"
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Datasetsüìö</heading>
                                    <!-- <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p> -->
                                </td>
                            </tr>
                        </tbody>
                    </table>
                
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/vqa.jpg" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://www.kaggle.com/datasets/bhavikardeshna/visual-question-answering-computer-vision-nlp" target="_blank">
                                        <papertitle>Visual Question Answering- Computer Vision & NLP</papertitle>
                                    </a>
                                    <br>
                                    The dataset is uploaded to <em>Kaggle</em>
                                    <br>
                                    <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/visual-question-answering-computer-vision-nlp" target="_blank">dataset</a></u> 
                                    <p>
                                        VQA is a multimodal task wherein, given an image and a natural language question related to the image, the objective is to produce a natural language answer correctly as output. VQA entails a wide range of sub-problems in both CV and NLP (such as object detection and recognition, scene classification, counting, and so on). Thus, it is considered an AI-complete task.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/yahoo.jpg" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://www.kaggle.com/datasets/bhavikardeshna/yahoo-email-classification" target="_blank">
                                        <papertitle>Yahoo! Answers Topic Classification
                                        </papertitle>
                                    </a>
                                    <br>
                                    The dataset is uploaded to <em>Kaggle</em>
                                    <br>
                                    <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/yahoo-email-classification" target="_blank">dataset</a></u> 
                                    <p>
                                        The Yahoo! Answers topic classification dataset is constructed using the 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000, and testing samples are 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/amazon-review.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://www.kaggle.com/datasets/bhavikardeshna/amazon-customerreviews-polarity" target="_blank">
                                        <papertitle>Amazon Customer-Reviews Polarity
                                        </papertitle>
                                    </a>
                                    <br>
                                    The dataset is uploaded to <em>Kaggle</em>
                                    <br>
                                    <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/amazon-customerreviews-polarity" target="_blank">dataset</a></u> 
                                    <p>
                                        The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. It supports text classification and sentiment-classification: The dataset is mainly used for text classification: given the content and the title, predict the correct star rating.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>
                    
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        Website <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                                        taken from <a href="https://jonbarron.info/">Jon Barron</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>