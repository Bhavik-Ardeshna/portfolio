<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Bhavik Ardeshna</title>

    <meta name="author" content="Bhavik Ardeshna">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!--<link rel="icon" type="image/png" href="images/waterloo_crest.png"> -->
</head>

<body style="background-color: rgb(216, 229, 231);">
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Bhavik Ardeshna</name>
                                    </p>
                                    <p> I work on cool stuff as an Applied Scientist at <a
                                            href="https://msturing.org/">Microsoft Turing</a> in Montreal.
                                    </p>
                                    <p>
                                        Before that, I graduated with an MMath in Computer Science from the <a
                                            href="https://www.uwaterloo.ca">University of Waterloo</a> where I worked on
                                        natural language processing and machine learning under the supervision of <a
                                            href="https://cs.uwaterloo.ca/~jimmylin/">Jimmy Lin</a> and <a
                                            href="https://cs.uwaterloo.ca/~ppoupart/">Pascal Poupart</a>. I had recieved
                                        the <a
                                            href="https://cs.uwaterloo.ca/current-graduate-students/funding-and-awards/david-r-cheriton-graduate-scholarship">David
                                            R. Cheriton Graduate Scholarship</a> at the University of Waterloo.
                                    </p>
                                    <p>During my time at UWaterloo, I was a research intern at <a
                                            href=https://www.mila.quebec"">Mila </a> from the fall of 2019 to the summer
                                            of 2020, where I worked with <a
                                            href="https://www.cs.mcgill.ca/~wlh/">William L.
                                            Hamilton</a> in
                                        collaboration with <a
                                            href="https://www.microsoft.com/en-us/research/lab/microsoft-research-montreal/projects/">Microsoft
                                            Research, Montreal</a> on using structured representations to develop
                                        generalizable policies for playing text-based games.</p>
                                    <p>
                                        Before that, I completed my BTech (Honors) in Information and Communication
                                        Technology with minor in Computational Science in 2018 from <a
                                            href="http://www.daiict.ac.in">DA-IICT</a>, Gandhinagar, India. I was a
                                        research intern at <a href="https://www.ntu.edu.sg">Nanyang Technological
                                            University, Singapore</a> in the winter of 2018 and spent the summer of 2017
                                        at the <a href="http://val.serc.iisc.ernet.in/valweb/">Video Analytics Lab</a>
                                        at the <a href="https://www.iisc.ac.in/">Indian Institute of Science,
                                            Bengaluru</a>.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:ardeshnabhavik@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="data/Bhavik_Ardeshna.pdf">Resume</a> &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/bhavik-ardeshna-a4494a1b0/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                                        <a href="https://github.com/Bhavik-Ardeshna" target="_blank">GitHub</a>&nbsp/&nbsp
                                        <a href="https://www.kaggle.com/bhavikardeshna" target="_blank">Kaggle</a>&nbsp/&nbsp 
                                        <a href="https://medium.com/@ardeshnabhavik" target="_blank">Medium</a>&nbsp/&nbsp 
                                        <a href="https://huggingface.co/bhavikardeshna" target="_blank">HuggingFaceü§ó</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="image/1-Edit.png"><img
                                            style="width:100%;max-width:100%;border-radius: 50%;border: 4px solid rgb(0, 0, 0);"
                                            alt="profile photo" src="image/profile-image.jpeg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publicationsüìù</heading>
                                    <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/GujQA.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arxiv.org/abs/2112.09866" target="_blank">
                                        <papertitle>GujaratiQASuite: Novel Resources for Gujarati Question-Answering System</papertitle>
                                    </a>
                                    <br>
                                    <strong>Bhavik Ardeshna<sup>*</sup></strong>, <a
                                        href="https://www.researchgate.net/profile/Hariom-Pandya-2" target="_blank">Hariom
                                        Pandya</a><sup>*</sup>, <a
                                        href="https://scholar.google.co.in/citations?user=aEkOFcUAAAAJ&hl=en" target="_blank">Brijesh
                                        Bhatt</a>
                                    <br>
                                    Passed ARR and Under Review in <em>Springer Journal</em>
                                    <br>
                                    <!-- <u><a href="data/GATA.bib">bibtex</a></u> <u><a
                                            href="https://github.com/xingdi-eric-yuan/GATA-public">code</a></u> -->
                                    <p>
                                        Gujarati Question Answering Dataset (GujQA), which is the baseline QA benchmark for the Gujarati language. Additionally, we have offered thorough assessments of the GujQA using a variety of linguistic criteria and have seen encouraging outcomes. We believe that GujQA, GujAdapter, and
                                        Gujwiki will not only advance the study of the Gujarati language‚Äôs understudied QA but also
                                        provide a door to the cross-lingual study involving the languages of the typologically varied domain
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/caledip.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arxiv.org/abs/2112.09866" target="_blank">
                                        <papertitle>Cascading Adaptors to Leverage English Data to Improve Performance
                                            of Question Answering for Low-Resource Languages
                                        </papertitle>
                                    </a>
                                    <br>
                                    <strong>Bhavik Ardeshna<sup>*</sup></strong>, <a
                                        href="https://www.researchgate.net/profile/Hariom-Pandya-2" target="_blank">Hariom
                                        Pandya</a><sup>*</sup>, <a
                                        href="https://scholar.google.co.in/citations?user=aEkOFcUAAAAJ&hl=en" target="_blank">Brijesh
                                        Bhatt</a>
                                    <br>
                                    Accepted as a Thesis-Paper at <em>ACL Anthology (ICON-22)</em>
                                    <br>
                                    <u><a href="data/caledip.bib">bibtex</a></u> <u><a
                                            href="https://github.com/Bhavik-Ardeshna/Question-Answering-for-Low-Resource-Languages" target="_blank">code</a></u> <u><a href="data/caledip.pdf">pdf</a></u>
                                    <p>
                                        We have investigated the efficacy of cascading adapters with transformer models to leverage high-resource language to improve the performance of low-resource languages on the question answering task. We trained four variants of adapter combina-
                                        tions for - Hindi, Arabic, German, Spanish, English, Vietnamese, and Simplified Chinese languages. We demonstrated that by using the transformer model with the multi-task adapters, the performance can be improved for the downstream task.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Articles & Blogs‚úç</heading>
                                    <p>
                                        I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/coqa.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arvrjourney.com/basic-intuition-of-conversational-question-answering-systems-cqa-cf79bb5fa1d6" target="_blank">
                                        <papertitle>Basic intuition of Conversational Question Answering Systems (CQA)</papertitle>
                                    </a>
                                    <br>
                                    Published in <em>Becoming Human: Artificial Intelligence Magazine & Medium</em>
                                    <br>
                                    <u><a href="https://becominghuman.ai/" target="_blank">magazine</a></u> <u><a
                                            href="https://arvrjourney.com/basic-intuition-of-conversational-question-answering-systems-cqa-cf79bb5fa1d6" target="_blank">blog</a></u>

                                    <br><br>
                                    <quote><em>‚ÄúWe‚Äôre no longer teaching people how to communicate with systems, we‚Äôre teaching systems to communicate with people.‚Äù</em></quote>
                                    <p>
                                        The researcher has been working to develop an array of intelligent dialogue system that not only matches or surpasses a human‚Äôs level in carrying out an interactive conversation but also answers questions on a variety of topics.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src="image/visiolinguistic.png" alt="fast-texture" width="160" height="120">
                                </td>
                                <td width="75%" valign="middle">
                                    <a href="https://arvrjourney.com/semantic-alignment-of-linguistic-and-visual-understanding-using-multi-modal-transformer-86d64919ecc2" target="_blank">
                                        <papertitle>Semantic Alignment of Linguistic and Visual Understanding using Multi-modal Transformer
                                        </papertitle>
                                    </a>
                                    <br>
                                    Published in <em>Becoming Human: Artificial Intelligence Magazine & Medium</em>
                                    <br>
                                    <u><a href="https://becominghuman.ai/" target="_blank">magazine</a></u> <u><a
                                            href="https://arvrjourney.com/semantic-alignment-of-linguistic-and-visual-understanding-using-multi-modal-transformer-86d64919ecc2" target="_blank">blog</a></u> 
                                    <br><br>
                                    <quote><em>‚ÄúAlso, they don‚Äôt understand ‚Äî writing is language. The use of language. The language to create image, the language to create drama. It requires a skill of learning how to use language.‚Äù</em></quote>
                                    <p>
                                        Vision-language tasks, such as image captioning, visual question answering, and visual commonsense reasoning, serve as rich test-beds for evaluating the reasoning capabilities of visually informed systems.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                        <tr>
                            <td style="padding:20px;width:100%;vertical-align:middle">
                                <heading>Datasetsüìö</heading>
                                <p>
                                    I'm broadly interested in natural language processing, visiolinguistic representations zero-shot learning. I have worked on low-resource language models, transformers, adapters, cross-lingual transfer, image-text feature alignment. I have also explored the conversational question answering(CQA), open-domain learning.
                                </p>
                            </td>
                        </tr>
                    </tbody>
                </table>
               
                <table
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>

                        <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                                <img src="image/vqa.jpg" alt="fast-texture" width="160" height="120">
                            </td>
                            <td width="75%" valign="middle">
                                <a href="https://www.kaggle.com/datasets/bhavikardeshna/visual-question-answering-computer-vision-nlp" target="_blank">
                                    <papertitle>Visual Question Answering- Computer Vision & NLP</papertitle>
                                </a>
                                <br>
                                The dataset is uploaded to <em>Kaggle</em>
                                <br>
                                <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/visual-question-answering-computer-vision-nlp" target="_blank">dataset</a></u> 
                                <p>
                                    VQA is a multimodal task wherein, given an image and a natural language question related to the image, the objective is to produce a natural language answer correctly as output. VQA entails a wide range of sub-problems in both CV and NLP (such as object detection and recognition, scene classification, counting, and so on). Thus, it is considered an AI-complete task.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                                <img src="image/yahoo.jpg" alt="fast-texture" width="160" height="120">
                            </td>
                            <td width="75%" valign="middle">
                                <a href="https://www.kaggle.com/datasets/bhavikardeshna/yahoo-email-classification" target="_blank">
                                    <papertitle>Yahoo! Answers Topic Classification
                                    </papertitle>
                                </a>
                                <br>
                                The dataset is uploaded to <em>Kaggle</em>
                                <br>
                                <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/yahoo-email-classification" target="_blank">dataset</a></u> 
                                <p>
                                    The Yahoo! Answers topic classification dataset is constructed using the 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000, and testing samples are 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.
                                </p>
                            </td>
                        </tr>

                        <tr>
                            <td style="padding:20px;width:25%;vertical-align:middle">
                                <img src="image/amazon-review.png" alt="fast-texture" width="160" height="120">
                            </td>
                            <td width="75%" valign="middle">
                                <a href="https://www.kaggle.com/datasets/bhavikardeshna/amazon-customerreviews-polarity" target="_blank">
                                    <papertitle>Amazon Customer-Reviews Polarity
                                    </papertitle>
                                </a>
                                <br>
                                The dataset is uploaded to <em>Kaggle</em>
                                <br>
                                <u><a href="https://www.kaggle.com/datasets/bhavikardeshna/amazon-customerreviews-polarity" target="_blank">dataset</a></u> 
                                <p>
                                    The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. It supports text classification and sentiment-classification: The dataset is mainly used for text classification: given the content and the title, predict the correct star rating.
                                </p>
                            </td>
                        </tr>

                    </tbody>
                </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        Website <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                                        taken from <a href="https://jonbarron.info/">Jon Barron</a>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>